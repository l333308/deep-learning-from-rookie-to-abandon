# 1. 参数的更新方法

# 回顾随机梯度下降法 SGD（stochastic gradient descent）
神经网络的目的是找到，使损失函数的值尽可能小的参数。这是一个很难的参数。
以探险家的故事类比：探险家在山谷里要找到最低的位置。但他要遵循两个原则：① 不用地图 ② 蒙着眼睛。
随机梯度下降法，就是根据地面的坡度前进。

# SGD的缺点
SGD容易理解，实现也容易，但是也有缺点。
f(x, y) = 1/20 * x^2 + y^2
这个函数的损失函数梯度方向，没有指向最小值方向，因此效率很低。呈“之”字型。

针对SGD的缺点，介绍以下三种替代方法：
- Momentum
- AdaGrad
- Adam
在上述这个函数里，这4个方法，对应的训练过程及效率，见图片4种方法-对比.png。

# Momentum
Momentum 物理中的动量。是SGD的改良版。

# AdaGrad
AdaGrad 是一个针对神经网络的梯度的元素进行学习率调整的方法。
学习率的调整，是根据元素的梯度大小来调整的。
梯度大的元素，学习率小；梯度小的元素，学习率大。

# Adam
Adam 是Momentum和AdaGrad的结合。

但是，（目前）并不存在能在所有问题中都表现良好的方法。这4种方法各有各的特点，都有各自擅长解决的问题和不擅长解决的问题。所以需要根据具体问题，选择合适的方法。

# 2. 权重的初始值
学习过程中，合理的权重初始值很重要。
之前实现的mnist数据集，以下是不同的初始值，学习过程。见不同权重 mnist学习过程.png

# 3. Batch Normalization
之前学到Normalization，是指把输入值进行正规化，把值限定到一定范围。如果这个范围是0到1，就称为归一化。
Batch Norm,就是以进行学习时的mini-batch为单位，进行正规化。

优势有以下几点：
- 加速学习（可以增大学习率）
- 抑制过拟合（降低Dropout等的必要性）
- 初始值的设定更加容易（对于初始值不用那么神经质）

初始值选得很好时，使用Batch Norm对于训练的优化效果不大。
但是，在选得很糟糕时，使用Batch Norm，效果会很好。见图片 Batch Norm效果.png

# 4 过拟合