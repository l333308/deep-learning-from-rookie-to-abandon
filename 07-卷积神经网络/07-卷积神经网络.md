# 全连接层
之前学到的，神经网络相邻层的所有节点，相互间都有连接，称为全连接层。用Affine层实现了全连接层。
全连接层有个问题：图像通常是高、长、通道方向上的3维形状，这个形状含有重要的空间信息。比如，空间上
邻近的像素为相似的值、RBG的各个通道之间分别有密切的关联性。向全连接层输入时，需要将3维数据拉平为
1维数据。无法利用与形状相关的信息。

# 通道数
- 彩色图像(RGB)的输入通道数为3 ：对应红(R)、绿(G)、蓝(B)三个颜色通道
- 黑白图像(灰度图)的输入通道数为1 ：只有一个亮度通道

# 滤波器 Filter（也称为卷积核或过滤器）
滤波器的形状决定了输出特征图的形状。有长度、宽度、深度三个维度。其中深度 = 输入特征图的深度。
滤波器的数量决定了输出特征图的通道数，两者相等。

# 特征图
卷积层的输入、输出，分别称为输入特征图、输出特征图。

# 卷积运算
输入特征图 * 滤波器 = 输出特征图

跟之前学到的矩阵乘法有相似性，但运算规则不同。
4*4矩阵A 3*3矩阵B 进行卷积运算，建一个3*3的框，在矩阵A上从左到右、从上到下滑动，每次计算框内元素与矩阵B对应元素乘积，再求和。
所以输出形状为2*2。
见图卷积运算 计算顺序.png，很好理解。

# 填充
见↑所述，输出结果的形状缩小了。为避免这种情况，在输入特征图的周围增加若干层，填充0，称为填充，使输出形状保持。

# 步幅（stride）
之前说到的框，移动距离为1格，称为步幅。步幅可以不为1.

# 池化层
池化是把输入特征图缩小高、长。
池化方式分为Max池化、Average池化。
池化的窗口大小、步幅可以设置。
池化后的窗口大小会和步幅设定成相同的值。比如，3 × 3的窗口的步幅会设为3，4 × 4的窗口的步幅会设为4。
见Max池化 2x2.png

# 层组合 实现CNN
见 简单CNN的网络构成.png

# CNN识别mnist数据集
见 train_convnet.py
每轮随机抽取5000图片，训练20轮。最终训练数据准确率99.5%,测试数据96.4%。
=== epoch:20, train acc:0.995, test acc:0.964 ===
相比误差反向传播法，提升还是有的。
第9600次 打印train acc : 0.9805, test acc : 0.9717

# 具有代表性的CNN
## LeNet
1998年被提出，是进行手写数字识别的网络。

## AlexNet
2012年被提出。

# 加深网络
5x5的矩阵，要通过卷积运算得到1x1的输出。滤波器大小的选择有多种方法：
1）5x5，一步得出结果，滤波器为1层，参数数量为25（5x5）
2）4x4,2x2，两步，2层，20（（4x4）+（2x2））
3）3x3，3x3，两步，2层，18（（3x3）+（3x3））
神经网络的层数加深，可以减少参数数量，且更深的层数，可以提高网络的能力范围。在前面的卷积层中，神经元会对边缘等简单的
形状有响应，随着层的加深，开始对纹理、物体部件等更加复杂的东西有响应。

# ImageNet
ImageNet是拥有超过100万张图像的数据集。