# 计算图解决实际问题
见 计算图求商品总价.png

从左到右，称为正向传播。从右到左，称为反向传播。
这里的反向传播，是为了计算苹果的价格变动，对于总价的影响程度有多大。视为求总价对于苹果价格的导数

# 链式法则 chain rule
先看复合函数，由多个函数复合而成的函数。
z = (x+y)^2,可拆分为以下两个函数
1) z = t^2
2) t = x+y
在求导时，该链式法则也试用。z对x的导数，就是z对t的导数乘以t对x的导数。
1）的导数是2t。推导：( (t+h)^2 - t^2 ) / h = 2t + h。h趋近于0。所以，z对t的导数是2t。
2）的导数是1
所以，z对x的导数是2t * 1 = 2(x+y)

# 反向传播
注意：此阶段的反向传播示例里，都有一个数字1.3.在实际神经网络，这个值会是由更上层的计算传递下来的梯度。
此处直接给出了1.3.
作者这里没有给出计算过程，也没有注释，差评。

## 加法节点的反向传播
layer_naive.py class AddLayer

见图5-11。10 + 5 = 15。
1) 正向传播时，将上游的值传递给下游。
2) 反向传播时，将下游的值乘以正向传播时的上游的值，然后传递给上游。

## 乘法节点的反向传播
layer_naive.py class MulLayer

z = x * y. x = 10, y = 5.
z对x的导数是y。z对y的导数是x。通过数值微分可以求出。
注意点：
反向传播到x，是1.3 * 5（x的翻转值）= 6.5
反向传播到y，是1.3 * 10（y的翻转值）= 13

图5-15，图5-17，这两张图以及前后文字说明，对于理解此块逻辑很有用。
接下来，用代码实现，加深理解。

buy_apple.py，实现图5-16,购买2个苹果的正向传播.
各个变量的导数，可由layer_naive的backward求出。

图5-17， 购买2*苹果 + 3*橘子，复杂一点的正向/反向传播。buy_apple_orange.py

# 梯度确认
学习了两种求梯度的方法：
1）数值微分，实现简单，计算耗费时间长
2）误差反向传播算法，实现复杂，容易出错
经常会比较两种方法的结果，以确认误差反向传播法的实现是否正确。
见gradient_check.py

回显结果：
W1:1.4867513930975084e-07
b1:3.3629294923711183e-09
W2:1.9021084343650196e-06
b2:1.5788888549743418e-07

误差虽然比书上说的-13维度要大，但也是很小的值。

# 使用误差反向传播算法进行学习
代码见train_neuralnet.py，准确率变化见识别准确率变化.png。
与第四章的训练相比，代码唯一区别是，使用了误差反向传播算法，见两个two_layer_net的gradient()。

代码执行5次，最终识别准确率能到0.98,相比第四章提升明显。回显分别为：
第9600次 打印train acc : 0.9800, test acc : 0.9701
第9600次 打印train acc : 0.9772, test acc : 0.9669
第9600次 打印train acc : 0.9770, test acc : 0.9672
第9600次 打印train acc : 0.9806, test acc : 0.9730
第9600次 打印train acc : 0.9805, test acc : 0.9717